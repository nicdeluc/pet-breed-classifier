{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3a3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import wandb\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084a45fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3680\n",
      "Test samples: 3669\n",
      "Size of the images: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # Normalization of ImageSet (necessary if we use a model pre-trained on ImageSet, such as ResNet-18)\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = OxfordIIITPet(\n",
    "    root=\"../data\",\n",
    "    split=\"trainval\",\n",
    "    transform=train_transform,\n",
    "    download=True\n",
    ")\n",
    "test_data = OxfordIIITPet(\n",
    "    root=\"../data\",\n",
    "    split=\"test\",\n",
    "    transform=test_transform,\n",
    "    download=True\n",
    ")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f'Size of the images: {train_data[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d52e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, dropout_p=0.5):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model and set it up for training:\n",
    "    replace the last fully-connected layer and freeze\n",
    "    the rest of the model.\n",
    "    \"\"\"\n",
    "    # Freeze all the layers in the base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    num_ftrs = model.fc.in_features\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Create a new fully-connected layer for our new classes\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "    ) \n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b90a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, dataloaders, loss_fn, optimizer, epochs=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    \"\"\" \n",
    "    # Create a dictionary to store training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    train_dataloader, test_dataloader = dataloaders\n",
    "\n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        ### Training Phase ###\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            # Move data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        ### Validation Phase ###\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        \n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        with torch.inference_mode():\n",
    "            for X, y in test_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                val_pred = model(X)\n",
    "                \n",
    "                # Calculate loss and accuracy\n",
    "                val_loss += loss_fn(val_pred, y).item()\n",
    "                val_acc += (val_pred.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "        # Calculate metrics over the whole validation set\n",
    "        val_loss /= len(test_dataloader)\n",
    "        val_acc /= len(test_dataloader.dataset)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch: {epoch+1} | \"\n",
    "              f\"Train loss: {train_loss:.4f} | \"\n",
    "              f\"Val loss: {val_loss:.4f} | \"\n",
    "              f\"Val acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc\n",
    "        })\n",
    "        \n",
    "    return history\n",
    "    \n",
    "def fine_tune_full(model, dataloaders, loss_fn, optimizers, epochs_head=5, epochs_full=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    optimizer_head, optimizer_full = optimizers\n",
    "    \n",
    "    # Train the head of the model\n",
    "    print('Training model\\'s head...')\n",
    "    history_head = fine_tune(model, dataloaders, loss_fn, optimizer_head, epochs=epochs_head, device=device)\n",
    "    \n",
    "    # Unfreeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    print('Training full model...')\n",
    "    # Train the full model\n",
    "    history_full = fine_tune(model, dataloaders, loss_fn, optimizer_full, epochs=epochs_full, device=device)\n",
    "    print('Finished Training')\n",
    "    \n",
    "    # Join the histories\n",
    "    history = {'train_loss': history_head['train_loss'] + history_full['train_loss'], \n",
    "               'val_loss': history_head['val_loss'] + history_full['val_loss'], \n",
    "               'val_acc': history_head['val_acc'] + history_full['val_acc']}\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e307ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for W&B sweep to find optimal hyperparameters\n",
    "sweep_config = {\n",
    "    'method': 'bayes',           # The search strategy: bayes, random, or grid\n",
    "    'metric': {\n",
    "        'name': 'val_acc' ,       \n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr_head': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'min': 0.00001,\n",
    "            'max': 0.0005\n",
    "        },\n",
    "        'lr_full': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.00001,\n",
    "            'max': 0.0005\n",
    "        },\n",
    "        'weight_decay_full': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'dropout_p': {\n",
    "            'values': [0.25, 0.5, 0.75]\n",
    "        },\n",
    "        'epochs_head': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 5,\n",
    "            'max': 15\n",
    "        },\n",
    "        'epochs_full': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 10,\n",
    "            'max': 30\n",
    "        }\n",
    "    }  \n",
    "}\n",
    "\n",
    "def sweep(sweep_config, model, dataloaders, loss_fn, device='cuda'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with wandb.init(entity = 'nicdeluc-learning', \n",
    "                    project = 'pet-breed-classification',\n",
    "                    config = sweep_config):\n",
    "        config = wandb.config\n",
    "\n",
    "        model = load_model(model, dropout=config.dropout_p)\n",
    "        \n",
    "        optim_head = optim.Adam(model.fc.parameters(), lr=config.lr_head)\n",
    "        optim_full = optim.Adam(model.parameters(), lr=config.lr_full)\n",
    "        optimizers = optim_head, optim_full\n",
    "        \n",
    "        history = fine_tune_full(model, \n",
    "                                 dataloaders, \n",
    "                                 loss_fn, \n",
    "                                 optimizers, \n",
    "                                 epochs_head=config.epochs_head, \n",
    "                                 epochs_full = config.epochs_full)\n",
    "        \n",
    "    return history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0bdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_clf_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
